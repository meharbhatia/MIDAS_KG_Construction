## Updates are here ##

Regarding submissions: (sorted alphabetically)

- ```new_6.csv``` is created by using spacyNER+rulebased , dataset input was cleaned dataset, removed isolated nodes (even with isolated nodes number of tupples were more or less same). Also when the error came, I replaced the new line character (```\n```) with ```<space>```.

- ```new_7.csv``` has same parameter as that of ```new_6``` just the difference is, it has been run on ```g050_Coref_Dataset.csv```

- ```new_8.csv``` has same parameter as that of ```new_6``` just the difference is, it has been run on ```g055_Coref_Dataset.csv```

- ```new_9.csv``` is the first csv file which is generated by using flair. Dataset used was g50 coref dataset. SpacyNER is not implemented and CD JJ POS tags are not removed from this. It gave a score of 12.

- ```news_10.csv``` is same as new_9 just the difference is dataset. It used cleaned dataset.

- ```new_12_withJjPosCdExtractor``` is generated from flair chunker + JJPOSCDExtractor, dataset used was g050. Score = 12.6

- ```new_13_withoutJJ_withPosCdExtractor.csv``` is same as new12, the difference is JJ are not extracted instead they are kept intact. Only POS and CD are extracted. Isolated triplets are removed. Also the triplets which has empty nodes or nodes with only one alphabet are removed. Dataset used is g050. Score 12.21.

- ```new_14_removedJJ_withPosCdExtractor.csv``` score = 12.24 | it is same as new_13 the difference is JJ are completely removed. Dataset was g050.

- ```new_15_removedJJ_withPosCdExtractor_drop_duplicates.csv``` score = 12.15 | removed duplicates from above file.

- ```new_16_removedJJPosCd_withNNExtractor.csv``` like new_15, it removes JJ, Pos and CD all together. Dataset used g050. First one to be calculated using chunks stored in the file. Took 5 mins to generate as compared to previous 20 mins (on server). Score = 12.41

- ```new_17_REDUCED_removedJJ_withPosCdExtractor.csv``` is build on new_14. 5k triplets. Reductions performed by ```reduced_triplets.py``` file. Score = 12.3

- ```new_18.csv``` made using modified rules. g050 dataset and g050 phrases. Score = 12.66

- ```submission_11_52.csv``` is the best file for the best triplets generated (22nd July) giving score of 11.52. The order of triplets kept in thic case is s2, r, s1. This follows the rule based mining technique ran on the coreference (g050) dataset. 

- ```submissionREDUCED_11_52.csv``` build on submission_11_52. Reductions performed by ```reduced_triplets.py``` file. Score = 12.01.

- ```trial_submission_coref.csv``` includes the triples when redundant triples have been removed from coref_triplets.csv. Triples which do not have (NN, NNS, NNP, CD) in any of the node. Gave a score - 11.82.

- ```trial_submission.csv``` includes the triples when redundant triples have been removed from new_7.csv. Triples which do not have (NN, NNS, NNP, CD) in any of the node. Gave a score - 11.78 .

- ``` wiki_file_trial.csv``` includes the triples from ```trial_submission.csv``` after passing through domain based test via Wikipedia2vec. Gave a score - 11.77. 
